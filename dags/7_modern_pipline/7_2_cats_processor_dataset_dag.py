"""DAG B (Dataset): cats_processor_dataset_dag

Consumer –Ω–∞ Airflow Datasets.

–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ —Å–æ–±—ã—Ç–∏—è Dataset (–∏–∑ producer DAG).

–ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: Dataset ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å–∏–≥–Ω–∞–ª "–¥–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã".
Consumer –∑–Ω–∞–µ—Ç —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ (bucket/prefix) –∏ —Å–∞–º –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª.
"""

from __future__ import annotations

import json
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List

from airflow.decorators import dag, task
from airflow.datasets import Dataset
from airflow.models import Variable
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook


MINIO_CONN_ID = Variable.get("MINIO_CONN_ID", default_var="minio")
POSTGRES_CONN_ID = Variable.get("POSTGRES_CONN_ID", default_var="postgres_data")
RAW_BUCKET = Variable.get("MINIO_BUCKET_CATS", default_var="cats")
RAW_PREFIX = Variable.get("MINIO_RAW_PREFIX", default_var="raw")

CATS_RAW_DATASET = Dataset(f"minio://{RAW_BUCKET}/{RAW_PREFIX}")
TARGET_TABLE = "cat_images"


@dag(
    dag_id="7_2_cats_processor_dataset_dag",
    description="Consumer (Dataset): –±–µ—Ä—ë—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª –∏–∑ MinIO -> transform -> Postgres",
    schedule=[CATS_RAW_DATASET],
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["cats", "minio", "postgres", "dataset", "consumer", "modern_pipeline","basic_pipeline"],
    default_args={
        "owner": "airflow",
        "retries": 1,
        "retry_delay": timedelta(minutes=3),
    },
)
def cats_processor_dataset_dag():
    @task
    def read_latest_from_minio() -> Dict[str, Any]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª –≤ MinIO (–ø–æ timestamp –≤ –∏–º–µ–Ω–∏) –∏ —á–∏—Ç–∞–µ—Ç –µ–≥–æ.

        Dataset —Å–æ–±—ã—Ç–∏–µ ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å–∏–≥–Ω–∞–ª "–∏–¥–∏ –ø—Ä–æ–≤–µ—Ä—å".
        –ú—ã –∑–Ω–∞–µ–º —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ: bucket=cats, prefix=raw/, —Ñ–∞–π–ª—ã —Å timestamp.
        """
        s3 = S3Hook(aws_conn_id=MINIO_CONN_ID)

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–ª—é—á–µ–π –≤ prefix
        keys = s3.list_keys(bucket_name=RAW_BUCKET, prefix=RAW_PREFIX) or []

        if not keys:
            raise FileNotFoundError(
                f"–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ s3://{RAW_BUCKET}/{RAW_PREFIX}. "
                "Producer DAG –¥–æ–ª–∂–µ–Ω —Å–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ."
            )

        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π (timestamp –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É)
        latest_key = sorted(keys)[-1]

        print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(keys)}")
        print(f"‚úÖ –í—ã–±—Ä–∞–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π: s3://{RAW_BUCKET}/{latest_key}")

        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        body = s3.read_key(key=latest_key, bucket_name=RAW_BUCKET)
        if body is None:
            raise FileNotFoundError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å s3://{RAW_BUCKET}/{latest_key}")

        raw = json.loads(body)
        print(f"üìä –ü—Ä–æ—á–∏—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(raw.get('items', []))}")

        return raw

    @task
    def transform(raw: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç raw –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Postgres."""
        items = raw.get("items")
        if not isinstance(items, list):
            raise ValueError(f"–û–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫ items, –ø–æ–ª—É—á–µ–Ω–æ: {type(items)}")

        load_dt = datetime.now(timezone.utc)

        result = []
        for item in items:
            if not isinstance(item, dict):
                continue

            cat_id = item.get("id")
            url = item.get("url")

            if not cat_id or not url:
                continue

            result.append({
                "id": cat_id,
                "url": url,
                "width": item.get("width"),
                "height": item.get("height"),
                "load_dt": load_dt,
            })

        print(f"‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(result)}")
        return result

    @task
    def load_to_postgres(rows: List[Dict[str, Any]]) -> int:
        """–°–æ–∑–¥–∞—ë—Ç —Ç–∞–±–ª–∏—Ü—É (–µ—Å–ª–∏ –Ω–µ—Ç) –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ Postgres."""
        if not rows:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return 0

        pg = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
        conn = pg.get_conn()
        cur = conn.cursor()

        # –°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (
                id TEXT PRIMARY KEY,
                url TEXT NOT NULL,
                width INTEGER,
                height INTEGER,
                load_dt TIMESTAMP NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)
        conn.commit()

        # Upsert –¥–∞–Ω–Ω—ã—Ö
        cur.executemany(f"""
            INSERT INTO {TARGET_TABLE} (id, url, width, height, load_dt)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (id) DO UPDATE SET
                url = EXCLUDED.url,
                width = EXCLUDED.width,
                height = EXCLUDED.height,
                load_dt = EXCLUDED.load_dt;
        """, [
            (r["id"], r["url"], r["width"], r["height"], r["load_dt"])
            for r in rows
        ])

        conn.commit()
        inserted = len(rows)

        cur.close()
        conn.close()

        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {inserted}")
        return inserted

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º pipeline
    raw_data = read_latest_from_minio()
    transformed = transform(raw_data)
    load_to_postgres(transformed)


# –ò–Ω—Å—Ç–∞–Ω—Ü–∏—Ä—É–µ–º DAG
cats_processor_dataset_dag()




